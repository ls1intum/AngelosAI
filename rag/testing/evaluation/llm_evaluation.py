import os
from dotenv import load_dotenv
from testing.test_data_models.qa_data import QAData
from testing.models.azure_testing_model import AzureTestingModel


class LlmEvaluation:
    def __init__(self, model: AzureTestingModel):
        """
        Initialize the LlmEvaluation class, setting up testing model
        """
        self.model = model


    def ask_llm_to_classify(self, qaData: QAData, answer):
        """
        Use the OpenAI Chat Completions API to evaluate whether the question was sufficiently answered.
        Returns a structured response indicating the correctness and missing facts (if any).
        """

        # Define the system message for consistency and clear instructions
        messages = [
            {
                "role": "system",
                "content": f"""
                You are a study advisor and your job is to evaluate responses to student questions. The evaluation is based on how well the provided answer matches the ground truth answer in terms of content. Focus on facts, not the structure or formulation.
                Provide your response in the following format:
                
                1. Correctness: [Correct, Partly Correct, Partly Incorrect, Incorrect]
                2. Missing Facts: List any missing facts or key details (if applicable).
                """
            },
            {
                "role": "user",
                "content": f"""
                Given the following question: "{qaData.question}", and the ground truth answer from study administration: {qaData.answer}, please determine if the following answer generated by a RAG system also answers the question correctly:
                {answer}

                Important: Focus on content and key facts. If all essential facts are included, the answer is correct, even if the phrasing is different. 
                Please follow the response format described above.
                """
            }
        ]

        try:
            response_message = self.model.complete(messages=messages)

        except Exception as e:
            # Handle any errors (e.g., network, rate-limiting)
            response_message = f"Error during LLM evaluation: {str(e)}"

        return response_message
    

    def evaluate_facts(self, qaData: QAData, answer):
        """
        Use the OpenAI Chat Completions API to evaluate which facts from the ground truth are present in the provided answer.
        Returns a vector with 1 (fact present) or 0 (fact missing) for each fact.
        """

        # Join the facts into a formatted list
        facts_list = "\n".join([f"{i + 1}. {fact}" for i, fact in enumerate(qaData.key_facts)])

        # Define the system message for consistency and clear instructions
        messages = [
            {
                "role": "system",
                "content": """
                You are an evaluator tasked with determining if certain key facts are present in a student's answer.
                You are given a list of facts, a question, and a provided answer. Your job is to evaluate whether
                each fact is present in the answer based on content, regardless of exact wording or phrasing.
                
                Respond with a binary vector (e.g. [1, 0, 1]) where:
                1 means the fact is present in the answer,
                0 means the fact is missing.
                """
            },
            {
                "role": "user",
                "content": f"""
                Given the following question: "{qaData.question}", and the list of key facts:

                {facts_list}

                Please determine if the following answer generated by a RAG system contains these key facts:

                {answer}

                Provide a binary vector (e.g., [1, 0, 1]) representing whether each fact is present (1) or missing (0) in the answer.
                """
            }
        ]
        score_percentage = 0

        try:
            response_message = self.model.complete(messages)

            print(response_message)

            # Convert the response into a list of integers (binary vector)
            binary_vector = [int(x) for x in response_message if x in ['0', '1']]

            # Calculate the percentage of correct facts
            correct_facts = sum(binary_vector)
            total_facts = len(qaData.key_facts)
            score_percentage = (correct_facts / total_facts) * 100

        except Exception as e:
            # Handle any errors (e.g., network, rate-limiting)
            response_message = f"Error during LLM evaluation: {str(e)}"

        return score_percentage